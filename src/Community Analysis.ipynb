{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import ast\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from networkx.algorithms.centrality import out_degree_centrality, betweenness_centrality, degree_centrality\n",
    "import ast\n",
    "\n",
    "# allowing the plot in line for seaborn\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# enabling outlines line in histogram \n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_community(G, id_com):\n",
    "    community_node = list()\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        if  G.nodes[node]['community'] != id_com:\n",
    "            community_node.append(node)\n",
    "    subgraph = G.subgraph(community_node)\n",
    "    return subgraph\n",
    "\n",
    "def community_detection(name, opt, typology):\n",
    "    #### READING GRAPH\n",
    "    ## OPT == 0 --> Garimella ELSE VAX/COVID\n",
    "    if opt == 0:\n",
    "        graph = nx.read_gml(f'{name}.gml')\n",
    "        multi = nx.read_gml(f'Multi_{name}.gml')\n",
    "    else:\n",
    "        graph = nx.read_gml(f'Final_Graph_{name}.gml')\n",
    "        multi = nx.read_gml(f'Final_MultiGraph_{name}.gml')\n",
    "    #### METIS\n",
    "    list_com_metis, set_com_metis, info, exe_time = get_communities(graph, 'Metis', typology)\n",
    "    mod_m = modularity(list_com_metis, graph, weight='weight')\n",
    "    cov_m = coverage(multi, set_com_metis)\n",
    "    log_write_com_result('Metis', info, mod_m, cov_m, exe_time, opt, typology, name)\n",
    "\n",
    "    label_node_communities(graph, list_com_metis, typology, name, opt)\n",
    "    return [list_com_metis, mod_m, cov_m], [0, 0, 0]\n",
    "\n",
    "def get_communities(G, alg, typology, k = 0, seed = 0):\n",
    "    raw_partition = list()\n",
    "    list_com = list()\n",
    "    set_com = set()\n",
    "    info = list()\n",
    "    start = 0\n",
    "    end = 0\n",
    "\n",
    "    if typology == 'sentiment':\n",
    "        weight = 'weightWithSentiment'\n",
    "    elif typology == 'topic':\n",
    "        weight = 'weightWithTopic'\n",
    "    elif typology == 'hybrid':\n",
    "        weight = 'Hibrid'\n",
    "    else:\n",
    "        weight = 'weight'\n",
    "\n",
    "    if alg == 'Metis':\n",
    "        for edge in G.edges(data=True):\n",
    "            G[edge[0]][edge[1]][weight] = int(G[edge[0]][edge[1]][weight])\n",
    "        start = time.time()\n",
    "        raw_partition = nxmetis.partition(G, 2, edge_weight=weight, node_weight=None, node_size=None)\n",
    "        end = time.time()\n",
    "        list_com, set_com = get_community_dict_and_set(raw_partition[1])\n",
    "\n",
    "        print(f'Lenght community 0: {len(raw_partition[1][0])}')\n",
    "        print(f'Lenght community 1: {len(raw_partition[1][1])}')\n",
    "\n",
    "        info = [len(raw_partition[1][0]), len(raw_partition[1][1])]\n",
    "    elif alg == 'Fluid':\n",
    "        communities: Dict[int, set] = dict()\n",
    "        start = time.time()\n",
    "        partitions = nx_comm.asyn_fluidc(G, k=k, seed=seed)\n",
    "        end = time.time()\n",
    "        for p in range(k):\n",
    "            communities[p] = next(partitions)\n",
    "        raw_partition.append(communities[0])\n",
    "        raw_partition.append(communities[1])\n",
    "        print(f'Lenght community 0: {len(raw_partition[0])}')\n",
    "        print(f'Lenght community 1: {len(raw_partition[1])}')\n",
    "\n",
    "        list_com, set_com = get_community_dict_and_set(raw_partition)\n",
    "\n",
    "        info = [len(raw_partition[0]), len(raw_partition[1])]\n",
    "    else:\n",
    "        print('Wrong algorithm name')\n",
    "        set_com = -1\n",
    "        list_com = -1\n",
    "        info = -1\n",
    "\n",
    "    return list_com, set_com, info, end-start\n",
    "\n",
    "\n",
    "def get_community_dict_and_set(list_com):\n",
    "    '''\n",
    "    From communities partitions returns a list of set\n",
    "    and dictionary of these\n",
    "    '''\n",
    "    com = 0\n",
    "    commun_dict = dict()\n",
    "    commun_list = list()\n",
    "    single_com = set()\n",
    "\n",
    "    for communities in list_com:\n",
    "        for member in communities:\n",
    "            commun_dict[member] = com\n",
    "            single_com.add(member)\n",
    "        com += 1\n",
    "        commun_list.append(single_com)\n",
    "        single_com = set()\n",
    "    return commun_dict, commun_list\n",
    "\n",
    "def modularity(partition, graph, weight='weight'):\n",
    "    if graph.is_directed():\n",
    "        raise TypeError(\"Bad graph type, use only non directed graph\")\n",
    "\n",
    "    inc = dict([])\n",
    "    deg = dict([])\n",
    "    links = graph.size(weight=weight)\n",
    "    if links == 0:\n",
    "        raise ValueError(\"A graph without link has an undefined modularity\")\n",
    "\n",
    "    for node in graph:\n",
    "        com = partition[node]\n",
    "        deg[com] = deg.get(com, 0.) + graph.degree(node, weight=weight)\n",
    "        for neighbor, datas in graph[node].items():\n",
    "            edge_weight = datas.get(weight, 1)\n",
    "            if partition[neighbor] == com:\n",
    "                if neighbor == node:\n",
    "                    inc[com] = inc.get(com, 0.) + float(edge_weight)\n",
    "                else:\n",
    "                    inc[com] = inc.get(com, 0.) + float(edge_weight) / 2.\n",
    "\n",
    "    res = 0.\n",
    "    for com in set(partition.values()):\n",
    "        res += (inc.get(com, 0.) / links) - \\\n",
    "               (deg.get(com, 0.) / (2. * links)) ** 2\n",
    "    return res\n",
    "\n",
    "\n",
    "def create_multi_graph(G):\n",
    "    G_multi = nx.MultiGraph()\n",
    "    for edge in G.edges(data = True):\n",
    "        weight = ceil(edge[2]['weightWithSentiment'])\n",
    "        for _ in range(weight):\n",
    "            G_multi.add_edge(edge[0], edge[1])\n",
    "\n",
    "    return G_multi\n",
    "\n",
    "def save_img(title, name): \n",
    "    plt.savefig(f'/Users/villons/Desktop/echo-chamers/src/analysis_images/{name}/{title}.png', \n",
    "                dpi = 300, quality = 95, format = 'png', pad_inches = 1000)\n",
    "    \n",
    "def community_pie_chart(graph, comm_type, name, study):\n",
    "    \n",
    "    communities = dict()\n",
    "    for node in graph.nodes(data=True):\n",
    "        if not(f'Comunità: {node[1][comm_type]}' in communities):\n",
    "            communities[f'Comunità: {node[1][comm_type]}'] = 1\n",
    "        else:\n",
    "            communities[f'Comunità: {node[1][comm_type]}'] += 1\n",
    "            \n",
    "            \n",
    "    com = list(communities.keys())\n",
    "    com_comp = list(communities.values())\n",
    "    colors = ['#b50000', '#0045b5']\n",
    "    if com[0] != 'Comunità: 1':\n",
    "        com[0], com[1] = com[1], com[0]\n",
    "        com_comp[0], com_comp[1] = com_comp[1], com_comp[0]\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(com_comp, labels = com, explode = [0, 0.1], shadow=True,\n",
    "        startangle = 90, colors = colors, wedgeprops={'edgecolor': 'black'}, autopct='%1.1f%%', textprops={'fontsize': 20})\n",
    "    ax.set_title(f'Suddivisione community per {name}', fontsize=15)\n",
    "    save_img(f'Suddivisione community per {name}', study)\n",
    "    plt.show() \n",
    "        \n",
    "def create_graph_df(graph):\n",
    "    graph_df = pd.DataFrame(columns=['username', 'sentiment', 'weightComm', 'sentimentComm', 'topicComm', 'hybridComm', 'tweetTopic'])\n",
    "    #graph_df = pd.DataFrame(columns=['username', 'sentiment', 'weightComm', 'sentimentComm'])\n",
    "    for node in tqdm(graph.nodes(data=True)):\n",
    "        new_row = [node[0], node[1]['sentiment'], node[1]['weightComm'], node[1]['sentimentComm'],\n",
    "                   node[1]['topicComm'], node[1]['hybridComm'], node[1]['tweetTopic']]\n",
    "        #new_row = [node[0], node[1]['sentiment'], node[1]['weightComm'], node[1]['sentimentComm']]\n",
    "        graph_df.loc[len(graph_df)] = new_row\n",
    "    return graph_df\n",
    "\n",
    "def add_to_dictionary(dictionary, key):\n",
    "    if key in dictionary:\n",
    "        dictionary[key] += 1\n",
    "    else:\n",
    "        dictionary[key] = 1\n",
    "    return dictionary\n",
    "\n",
    "def plot_degree_distribution(graph, name, opt):\n",
    "    degree_distribution = pd.DataFrame(columns=['degree_value', 'community', 'towards', 'placeholder'])\n",
    "    colors = ['#b50000', '#0045b5']\n",
    "        \n",
    "    for node in tqdm(graph.nodes(data=True)):\n",
    "        if opt == 0:\n",
    "            title = f'Distribuzione per {name}'\n",
    "                \n",
    "            out_edges = list(graph.out_edges(node[0], data=True))\n",
    "            in_edges = list(graph.in_edges(node[0], data=True))\n",
    "            out_degree_values = 0\n",
    "            in_degree_values = 0\n",
    "            \n",
    "            comm = node[1]['sentimentComm']\n",
    "            \n",
    "            for edge in out_edges:\n",
    "                out_degree_values += edge[2]['weight']\n",
    "\n",
    "            for edge in in_edges:\n",
    "                in_degree_values += edge[2]['weight']\n",
    "\n",
    "            degree_distribution.loc[len(degree_distribution)] = [out_degree_values, comm, 'outdegree', '']\n",
    "            degree_distribution.loc[len(degree_distribution)] = [in_degree_values, comm, 'indegree', '']\n",
    "        else:\n",
    "            graph_name = graph.name.split('graph')[1]\n",
    "            title = f'Distribuzione per {graph_name}'\n",
    "            edges = list(graph.edges(node[0], data=True))\n",
    "            edge_degree = 0\n",
    "            comm = node[1]['weightComm']\n",
    "            for edge in edges:\n",
    "                edge_degree += edge[2]['weight']\n",
    "            degree_distribution.loc[len(degree_distribution)] = [edge_degree, comm, 'total_degree', '']\n",
    "            \n",
    "    degree_value = degree_distribution['degree_value']\n",
    "    removed_outliers = degree_value.between(degree_value.quantile(.25), degree_value.quantile(0.85))\n",
    "    index_names = degree_distribution[~removed_outliers].index\n",
    "    degree_distribution.drop(index_names, inplace=True)\n",
    "    degree_distribution['degree_value'] = degree_distribution['degree_value'].apply(lambda x: float(x))\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    if opt == 1:\n",
    "        x_value = 'placeholder'\n",
    "    else:\n",
    "        x_value = 'towards'\n",
    "    \n",
    "    ax = sns.violinplot(x=x_value, y='degree_value', hue='community', data=degree_distribution, split=True, palette=['blue', 'red'], scale='count', bw=0.2)\n",
    "    ax.set_title(title, fontsize=15)\n",
    "    ax.set_xlabel('Distribuzione di grado', fontsize=15)\n",
    "    ax.set_ylabel('Numero di tweets', fontsize=15)\n",
    "    ax.legend(title='Community', prop={\"size\":15})\n",
    "    ax.xaxis.set_tick_params(labelsize=15)\n",
    "    ax.yaxis.set_tick_params(labelsize=15)\n",
    "    save_img(title, name)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_sentiment_distribution(compact_graph, name, sentiment_df):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax = sns.distplot(sentiment_df['sentiment'], hist=False, label='Sentimento su intero grafo')\n",
    "    title = f'Distribuzione sentiment su grafo {name}'\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Score sentimento')\n",
    "    ax.set_ylabel('Distribuzione probabilità')\n",
    "    ax.legend()\n",
    "    save_img(title, name)\n",
    "    plt.show()\n",
    "    \n",
    "    columns = sentiment_df.columns[2:len(sentiment_df.columns)-1]\n",
    "    #columns = sentiment_df.columns[2:len(sentiment_df.columns)]\n",
    "    \n",
    "    titles = ['strutturale', 'sentiment', 'topic', 'hybrid']\n",
    "    index = 0\n",
    "    print(columns)\n",
    "    for column in columns:\n",
    "        title = f'Distribuzione sentimento nelle communities dato approccio {titles[index]}'\n",
    "        sentiment_df_weight_0 = sentiment_df.query(f'{column} == 0')\n",
    "        sentiment_df_weight_1 = sentiment_df.query(f'{column} == 1')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax = sns.distplot(sentiment_df_weight_0['sentiment'], color='b', hist=False, label='Community 0')\n",
    "        ax = sns.distplot(sentiment_df_weight_1['sentiment'], color='r', hist=False, label='Community 1')\n",
    "        ax.set_title(title, fontsize=15)\n",
    "        ax.set_xlabel('Score sentimento', fontsize=19)\n",
    "        ax.set_ylabel('Distribuzione probabilità', fontsize=19)\n",
    "        ax.xaxis.set_tick_params(labelsize=17)\n",
    "        ax.yaxis.set_tick_params(labelsize=17)\n",
    "        ax.legend(prop={\"size\":19})\n",
    "        save_img(title, name)\n",
    "        plt.show()\n",
    "        index += 1\n",
    "        \n",
    "def delete_outliers(dataset, field):\n",
    "    \n",
    "    dataset_value = dataset[field]\n",
    "    removed_outliers = dataset_value.between(dataset_value.quantile(.25), dataset_value.quantile(0.85))\n",
    "    index_names = dataset[~removed_outliers].index\n",
    "    dataset = dataset.drop(index_names)\n",
    "    return dataset\n",
    "    \n",
    "def plot_snsdist(metadata, field):\n",
    "    metadata_no_out = delete_outliers(metadata, field)\n",
    "    fig, ax = plt.subplots()\n",
    "    com_0 = metadata_no_out.query('community == \"0\"')\n",
    "    com_1 = metadata_no_out.query('community == \"1\"')\n",
    "    \n",
    "    ax = sns.distplot(com_0[field], hist=False, label=f'{field} community 0')\n",
    "    ax = sns.distplot(com_1[field], hist=False, label=f'{field} community 1')\n",
    "    \n",
    "    title = f'Valutazione {field}'\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(f'{field}')\n",
    "    ax.set_ylabel(f'Distribuzione di probabiltà')\n",
    "    ax.legend()\n",
    "    save_img(title, 'COVID-19')\n",
    "    plt.show()\n",
    "    \n",
    "def get_metadata(opt = 0):\n",
    "    files = os.listdir()\n",
    "    \n",
    "    #if 'complete_metadata.csv' not in files:\n",
    "    if 'complete_metadata_vecchi.csv' not in files:\n",
    "        medatada_df = pd.DataFrame(columns=['username', 'created', 'defaultProfile',\n",
    "                                            'DefaultImage', 'description', 'numLikes', \n",
    "                                            'numFollowers', 'numFollowing', 'numGroups', \n",
    "                                            'location', 'numStatuses', 'verified', 'community',\n",
    "                                            'sentiment', 'placeholder'])\n",
    "        \n",
    "        not_useful = ['sentiment', 'sentimentComm', 'topicComm', 'hybridComm', 'weightComm', 'tweetTopic']\n",
    "        for node in tqdm(covid_complete.nodes(data=True)):\n",
    "            row = [node[0]]\n",
    "            \n",
    "            if 'description' in covid_metadata.nodes[node[0]]:\n",
    "                for medatada in covid_metadata.nodes[node[0]]:\n",
    "                    if medatada != 'sentimentComm' and medatada not in not_useful:\n",
    "                        row += [covid_metadata.nodes[node[0]][medatada]]\n",
    "            else:\n",
    "                row += [np.nan]*(len(medatada_df.columns)-4)\n",
    "\n",
    "            row += [node[1]['sentimentComm']]\n",
    "            row += [node[1]['sentiment']]\n",
    "            row += ['']\n",
    "            medatada_df.loc[len(medatada_df)] = row\n",
    "        medatada_df.to_csv('./complete_metadata_enrich.csv', index=False, encoding='utf-8')\n",
    "    else:\n",
    "        if opt == 0:\n",
    "            #medatada_df = pd.read_csv('./complete_metadata_vecchi.csv')\n",
    "            medatada_df = pd.read_csv('./complete_metadata_prima_di_infame.csv')\n",
    "        else:\n",
    "            medatada_df = pd.read_csv('./complete_metadata_enrich.csv')\n",
    "    \n",
    "    for field in ['numFollowers', 'numLikes', 'numFollowing', 'numGroups', 'numStatuses']:\n",
    "        medatada_df[field] = medatada_df[field].apply(lambda x: float(x))\n",
    "    \n",
    "    medatada_df['placeholder'] = [' ']*len(medatada_df)\n",
    "    medatada_df.dropna(axis='index', how='any', inplace=True)\n",
    "    \n",
    "    medatada_df['created'] = pd.to_datetime(medatada_df['created'], format='%Y-%m-%d')\n",
    "    \n",
    "    return medatada_df\n",
    "\n",
    "\n",
    "def verified_pie_chart(community, metadata_df):\n",
    "    comm = metadata.query(f'community == {community}')\n",
    "    verified_dict = dict(comm.verified.value_counts())\n",
    "    del verified_dict['-1']\n",
    "    verified_dict = dict(sorted(verified_dict.items()))\n",
    "    title = f'Account verificati per la community {community}'\n",
    "    \n",
    "    verified = list(verified_dict.keys())\n",
    "    print(verified)\n",
    "    \n",
    "    if verified[0] == 'False':\n",
    "        verified[0] = 'Non Verificato'\n",
    "        verified[1] = 'Verificato'\n",
    "    else:\n",
    "        verified[1] = 'Non Verificato'\n",
    "        verified[0] = 'Verificato'\n",
    "    \n",
    "    \n",
    "    count_ver = list(verified_dict.values())\n",
    "    # colors = ['#0045b5', '#b50000']\n",
    "    \n",
    "    print(verified)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(count_ver, labels = verified, explode = [0, 0.1], shadow=True,\n",
    "        startangle = 90, wedgeprops={'edgecolor': 'black'}, autopct='%1.1f%%', textprops={'fontsize': 20})\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    save_img(title, 'COVID-19')\n",
    "    plt.show() \n",
    "    \n",
    "def default_pie_chart(community, metadata_df, element, opt = 0):\n",
    "    comm = metadata.query(f'community == {community}')\n",
    "    if element == 'defaultProfile':\n",
    "        default_dict = dict(comm.defaultProfile.value_counts())\n",
    "        title = f'Profilo di default per utenti community {community}'\n",
    "    elif element == 'DefaultImage':\n",
    "        default_dict = dict(comm.DefaultImage.value_counts())\n",
    "        title = f'Immagine di default per utenti community {community}'\n",
    "        \n",
    "    if opt == 1:\n",
    "        title = f'{title} dati account 2020'\n",
    "    \n",
    "    if '-1' in default_dict:\n",
    "        del default_dict['-1']\n",
    "    default_dict = dict(sorted(default_dict.items()))\n",
    "    \n",
    "    verified = list(default_dict.keys())\n",
    "    count_ver = list(default_dict.values())\n",
    "    # colors = ['#0045b5', '#b50000']\n",
    "    \n",
    "    if verified[0] == 'False':\n",
    "        verified[0] = 'Default'\n",
    "        verified[1] = 'Personale'\n",
    "    else:\n",
    "        verified[1] = 'Default'\n",
    "        verified[0] = 'Personale'\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(count_ver, labels = verified, explode = [0, 0.1], shadow=True,\n",
    "        startangle = 90, wedgeprops={'edgecolor': 'black'}, autopct='%1.1f%%', textprops={'fontsize': 20})\n",
    "    ax.set_title(title, fontsize=30)\n",
    "    ax.xaxis.set_tick_params(labelsize=21)\n",
    "    ax.yaxis.set_tick_params(labelsize=21)\n",
    "    save_img(title, 'COVID-19')\n",
    "    plt.show() \n",
    "    \n",
    "def topic_distribution(graph_df, name, opt=0):\n",
    "    \n",
    "    titles = ['strutturale', 'sentiment', 'topic', 'hybrid']\n",
    "    cont = 0\n",
    "    \n",
    "    comm_types = list(graph_df.columns)[2:6]\n",
    "    \n",
    "    for comm_type in comm_types:\n",
    "        topic_cont = dict()\n",
    "        topic_cont_0 = dict()\n",
    "        topic_cont_1 = dict()\n",
    "        for _, row in graph_df.iterrows():\n",
    "            topics = ast.literal_eval(row['tweetTopic'])\n",
    "            community = row[f'{comm_type}']\n",
    "            for topic in topics:\n",
    "                topic_cont = add_to_dictionary(topic_cont, topic)\n",
    "                if community == 0:\n",
    "                    topic_cont_0 = add_to_dictionary(topic_cont_0, topic)\n",
    "                else:\n",
    "                    topic_cont_1 = add_to_dictionary(topic_cont_1, topic)\n",
    "                    \n",
    "                \n",
    "        topic_cont = dict(sorted(topic_cont.items()))\n",
    "        topic_cont_0 = dict(sorted(topic_cont_0.items()))\n",
    "        topic_cont_1 = dict(sorted(topic_cont_1.items()))\n",
    "\n",
    "        if opt == 0:\n",
    "            title = f'Distribuzione topic con approccio {titles[cont]}'\n",
    "        else:\n",
    "            title = f'Distribuzione topic con approccio {titles[cont]} - {opt} link prediction'\n",
    "\n",
    "        width = 0.25\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        x_indexes = np.arange(1, len(topic_cont)+1)\n",
    "        ax.bar(x_indexes - width, topic_cont.values(), width = width, color = '#444444', label = 'Su tutto il grafo')\n",
    "        ax.bar(x_indexes, topic_cont_0.values(), width = width, color = '#020ee8', label = 'Community 0')\n",
    "        ax.bar(x_indexes + width, topic_cont_1.values(), width = width, color = '#e00707', label = 'Community 1')\n",
    "        plt.xticks(ticks=x_indexes, labels=x_indexes)\n",
    "        ax.legend(prop={\"size\":25})\n",
    "        ax.set_xlabel('ID topic', fontsize=30)\n",
    "        ax.set_ylabel('Utenti associati ai topic', fontsize=30)\n",
    "        ax.set_title(title, fontsize=25)\n",
    "        ax.xaxis.set_tick_params(labelsize=20)\n",
    "        ax.yaxis.set_tick_params(labelsize=20)\n",
    "        \n",
    "        cont += 1\n",
    "\n",
    "        save_img(title, name)\n",
    "        plt.show()\n",
    "        \n",
    "def plot_hist_2020(meta_2020):\n",
    "    com_0 = meta_2020.query('community == 0')\n",
    "    com_1 = meta_2020.query('community == 1')\n",
    "    \n",
    "    colors = ['#0045b5', '#b50000']\n",
    "    \n",
    "    title = 'Numerosità account creati nel 2020 in funzione delle community'\n",
    "    fig, ax = plt.subplots()\n",
    "    x_indexes = np.arange(0, 2)\n",
    "    ax.bar(x_indexes, [len(com_0), len(com_1)], width = 0.5, color=colors)\n",
    "    plt.xticks(ticks=x_indexes, labels=x_indexes)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Community', fontsize=20)\n",
    "    ax.set_ylabel('# Account creati nel 2020', fontsize=20)\n",
    "    ax.set_title(title, fontsize=30)\n",
    "    prop={\"size\":30}\n",
    "    ax.xaxis.set_tick_params(labelsize=21)\n",
    "    ax.yaxis.set_tick_params(labelsize=21)\n",
    "    save_img(title, 'COVID-19')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_join_plot(field1, fiel2, metadata, title):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(field1)\n",
    "    ax.set_ylabel(fiel2)\n",
    "    ax.set_title(title)\n",
    "    ax = sns.jointplot(x=field1, y=fiel2, data=metadata, color='green', kind='reg')\n",
    "    #save_img(title, 'COVID-19')\n",
    "    plt.show()\n",
    "\n",
    "def delete_outliers(df, field):\n",
    "    values = df[field]\n",
    "    removed_outliers = values.between(values.quantile(.25), values.quantile(0.75))\n",
    "    index_names = df[~removed_outliers].index\n",
    "    df.drop(index_names, inplace=True)\n",
    "    return df\n",
    "    \n",
    "def join_plot(field1, field2, metadata, title):\n",
    "    \n",
    "    metadata_df = delete_outliers(field1, metadata)\n",
    "    metadata_df = delete_outliers(field2, metadata)\n",
    "    \n",
    "    comm_0 = metadata_df.query('community == \"0\"')\n",
    "    comm_1 = metadata_df.query('community == \"1\"')\n",
    "\n",
    "    plot_join_plot(field1, field2, comm_0, title)\n",
    "    plot_join_plot(field1, field2, comm_1, title)\n",
    "    \n",
    "def delete_url(tweet):\n",
    "    for i in range(len(tweet)):\n",
    "        tweet[i] = re.sub(r'\\b(http:|www\\.)(?:[^\\s,.!?]|[,.!?](?!\\s))+', '', tweet[i])\n",
    "        tweet[i] = re.sub(r\"http\\S+\", '', tweet[i])\n",
    "        tweet[i] = re.sub(r'http:\\\\*/\\\\*/.*?\\s', '', tweet[i])\n",
    "        tweet[i] = re.sub(r'https:\\\\*/\\\\*/.*?\\s', '', tweet[i])\n",
    "        tweet[i] = re.sub(r\"twitter.(\\w+)\", ' ', tweet[i], flags=re.MULTILINE)\n",
    "        tweet[i] = re.sub(r'://(?:[^\\s,.!?]|[,.!?](?!\\s))+', '', tweet[i])\n",
    "        tweet[i] = re.sub(r\"://\", ' ', tweet[i], flags=re.MULTILINE)\n",
    "        tweet[i] = re.sub(r\"/(\\w+)\", ' ', tweet[i], flags=re.MULTILINE)\n",
    "        tweet[i] = re.sub(r\"#\", '', tweet[i])\n",
    "        tweet[i] = re.sub(r'陈秋实(?:[^\\s,.!?]|[,.!?](?!\\s))+', '', tweet[i])\n",
    "        tweet[i] = re.sub(r'full(?:[^\\s,.!?]|[,.!?](?!\\s))+', '', tweet[i])\n",
    "        tweet[i] = re.sub(r'utm_source(?:[^\\s,.!?]|[,.!?](?!\\s))+', '', tweet[i])\n",
    "        tweet[i] = re.sub(r'utm_medium=(?:[^\\s,.!?]|[,.!?](?!\\s))+', '', tweet[i])\n",
    "        tweet[i] = re.sub(r'=social(?:[^\\s,.!?]|[,.!?](?!\\s))+', '', tweet[i])\n",
    "        tweet[i] = re.sub(r'=web&(?:[^\\s,.!?]|[,.!?](?!\\s))+', '', tweet[i])\n",
    "        tweet[i] = re.sub(r'utm_campaign(?:[^\\s,.!?]|[,.!?](?!\\s))+', '', tweet[i])\n",
    "        tweet[i] = re.sub(r'.html', '', tweet[i])\n",
    "        tweet[i] = re.sub(r'·ðŸ', '', tweet[i])\n",
    "        tweet[i] = re.sub(r'°ðŸ', '', tweet[i])\n",
    "        \n",
    "        if '°ðŸ' in tweet[i]:\n",
    "            tweet[i].replace('°ðŸ', '')\n",
    "    return tweet\n",
    "\n",
    "def word_cloud(words, mask_id):\n",
    "    \n",
    "    if mask_id == -1 or mask_id == 0:\n",
    "        papers_mask = np.array(Image.open(\"./twitter_mask_0.png\"))\n",
    "    else:\n",
    "        papers_mask = np.array(Image.open(\"./twitter_mask_1.png\"))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update(['NoDescription', 'https co', 'ðŸ', 'ãƒ'])\n",
    "    wordcloud = WordCloud(width=800, height=400, stopwords=stopwords, background_color=\"white\", \n",
    "                          mode=\"RGBA\", max_words=1000, mask = papers_mask).generate(words)\n",
    "\n",
    "    image_colors = ImageColorGenerator(papers_mask)\n",
    "    \n",
    "    return wordcloud, image_colors\n",
    "\n",
    "def preprocessing_summary(summary):\n",
    "    #with open(\"../../preprocessing/stopwords.txt\", \"rb\") as fp:\n",
    "    #    stop_words = pickle.load(fp)\n",
    "    #stop_words.append('NoDescription')\n",
    "    #stop_words.append('We')\n",
    "    #stop_words.append('new')\n",
    "    \n",
    "    stop_words = {'NoDescription', 'We', 'new', 'New', 'love', 'Love', \n",
    "                  'No', 'the', 'The', 'news', 'News', 'WWG', '1WGA', 'ðŸ', \n",
    "                  'ãƒ', '°ðŸ', 'Retired', 'Journalist', 'world', 'Former', 'Writer',\n",
    "                 'Twitter', 'people', 'politic', '°ðŸ ', ' °ðŸ ', ' °ðŸ', 'I\\'m', '°ðŸ',\n",
    "                 'politics', 'life', 'ºðŸ', ' ºðŸ', 'ºðŸ ', ' ºðŸ ', 'Editor', 'time',\n",
    "                 'Tweet', 'alum', 'Tweet', 'live', 'Global', 'music', 'If', 'now',\n",
    "                 'not endorsement', 'Follow', 'us', 'now', 'one', 'business'}\n",
    "    summary = delete_url(summary)\n",
    "    tokening = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    summary_series = pd.Series(summary)\n",
    "    summary_tokenized = summary_series.apply(tokening.tokenize)\n",
    "    for sentence in range(len(summary_tokenized)):\n",
    "        #for token in summary_tokenized[sentence]:\n",
    "        #    if 'new' in token:\n",
    "        #        print(f'prima{token}dopo')\n",
    "        #        print(summary_tokenized[sentence])\n",
    "        #        print()\n",
    "                \n",
    "        not_number = [token for token in summary_tokenized[sentence] if not token.isdigit()]\n",
    "        summary_tokenized[sentence] = not_number\n",
    "    summary_tokenized_stop = summary_tokenized.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    list_done = list(summary_tokenized_stop)\n",
    "    list_done = [item for sublist in list_done for item in sublist]\n",
    "    return list_done\n",
    "\n",
    "def summary_word_cloud(meta_df, title, opt):\n",
    "    summary = list()\n",
    "    cont = 0\n",
    "    \n",
    "    for _, row in meta_df.iterrows():\n",
    "        row['description'] = row['description'].replace('°ðŸ', '').replace('ºðŸ', '')\n",
    "        summary.append(row['description'])\n",
    "    summ_clean = preprocessing_summary(summary)\n",
    "    \n",
    "    print()\n",
    "            \n",
    "    wordcloud, colors = word_cloud(' '.join(summ_clean), opt - 1)\n",
    "    fig, ax = plt.subplots(figsize=[20,20])\n",
    "        \n",
    "    plt.title(title)\n",
    "    ax.axis(\"off\")\n",
    "    if opt == 0:\n",
    "        ax.imshow(wordcloud.recolor(color_func=grey_color_func), interpolation=\"bilinear\")\n",
    "    elif opt == 1:\n",
    "        ax.imshow(wordcloud.recolor(color_func=blue_color_func), interpolation=\"bilinear\")\n",
    "    elif opt == 2:\n",
    "        ax.imshow(wordcloud.recolor(color_func=red_color_func), interpolation=\"bilinear\")\n",
    "    save_img(title, 'COVID-19')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# 213, 82, 42 --> blue\n",
    "# 213, 63, 5 --> black\n",
    "# 0, 98, 44 --> red\n",
    "\n",
    "# for total\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(213, 63%, 5%)\"\n",
    "\n",
    "# for community 0\n",
    "def red_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(0, 98%, 44%)\"\n",
    "\n",
    "# for community 1\n",
    "def blue_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(213, 82%, 42%)\"\n",
    "\n",
    "\n",
    "def extract_community(G, id_com, type_com):\n",
    "    community_node = list()\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        if  G.nodes[node][type_com] == id_com:\n",
    "            community_node.append(node)\n",
    "    subgraph = G.subgraph(community_node)\n",
    "    return subgraph\n",
    "\n",
    "\n",
    "def compute_degree_and_betweenness(graph, opt):\n",
    "    graph_multi = create_direct_multigraph(graph)\n",
    "    print(nx.info(graph_multi))\n",
    "    \n",
    "    if opt == 0:\n",
    "        degree = degree_centrality(graph_multi)\n",
    "    elif opt == 1:\n",
    "        degree = out_degree_centrality(graph_multi)\n",
    "    \n",
    "    degree_sorted = sorted(degree.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(degree_sorted[0:10])\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    k = int(len(graph)*0.6)\n",
    "    betweenness = betweenness_centrality(graph, k = k, normalized = True, weight = 'weight')\n",
    "    betweenness_sorted = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(betweenness_sorted[0:10])\n",
    "    print()\n",
    "    \n",
    "def create_direct_multigraph(graph):\n",
    "    G_multi = nx.MultiDiGraph()\n",
    "    for edge in graph.edges(data = True):\n",
    "        weight = edge[2]['weight']\n",
    "        for _ in range(int(weight)):\n",
    "            G_multi.add_edge(edge[0], edge[1])\n",
    "    \n",
    "    print(nx.info(G_multi))\n",
    "    return G_multi\n",
    "\n",
    "def add_to_dict(total, partial):\n",
    "    partial = ast.literal_eval(partial)\n",
    "    for element in partial:\n",
    "        #if element == 0:\n",
    "        if element in total:\n",
    "            total[element] += 1\n",
    "        else:\n",
    "            total[element] = 1\n",
    "    return total\n",
    "\n",
    "\n",
    "def plot_topic_ver_or_not(covid_compact, covid_metadata, opt):    \n",
    "    com_0_verified = dict()\n",
    "    com_0_not_verified = dict()\n",
    "\n",
    "    com_1_verified = dict()\n",
    "    com_1_not_verified = dict()\n",
    "\n",
    "\n",
    "    for node in covid_metadata.nodes(data=True):\n",
    "        if node[1] != {}:\n",
    "            if covid_compact.nodes[node[0]]['sentimentComm'] == 0:\n",
    "                if node[1]['verified'] == 'True':\n",
    "                    com_0_verified = add_to_dict(com_0_verified, covid_compact.nodes[node[0]]['tweetTopic'])\n",
    "                else:\n",
    "                    com_0_not_verified = add_to_dict(com_0_not_verified, covid_compact.nodes[node[0]]['tweetTopic'])\n",
    "            else:\n",
    "                if node[1]['verified'] == 'True':\n",
    "                    com_1_verified = add_to_dict(com_1_verified, covid_compact.nodes[node[0]]['tweetTopic'])\n",
    "                else:\n",
    "                    com_1_not_verified = add_to_dict(com_1_not_verified, covid_compact.nodes[node[0]]['tweetTopic'])\n",
    "    # verified\n",
    "    if opt == 0:\n",
    "        topic_cont_0 = dict(sorted(com_0_verified.items()))\n",
    "        topic_cont_1 = dict(sorted(com_1_verified.items()))\n",
    "        title = 'Distribuzione utenti verificati sui topic'\n",
    "    # not verified\n",
    "    elif opt == 1:\n",
    "        topic_cont_0 = dict(sorted(com_0_not_verified.items()))\n",
    "        topic_cont_1 = dict(sorted(com_1_not_verified.items()))\n",
    "        title = 'Distribuzione utenti verificati sui topic'\n",
    "\n",
    "    #width = 0.2\n",
    "    width = 0.33\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    x_indexes = np.arange(1, len(topic_cont_0)+1)\n",
    "    ax.bar(x_indexes-width, topic_cont_0.values(), width = width, color = '#020ee8', label = 'Community 0')\n",
    "    ax.bar(x_indexes, topic_cont_1.values(), width = width, color = '#e00707', label = 'Community 1')\n",
    "    plt.xticks(ticks=x_indexes, labels=x_indexes)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('ID topic')\n",
    "    ax.set_ylabel('Utenti associati ai topics')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    save_img(title, 'COVID-19')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_sent_ver_or_not(covid_compact, covid_metadata, opt):    \n",
    "    com_0_verified = list()\n",
    "    com_0_not_verified = list()\n",
    "\n",
    "    com_1_verified = list()\n",
    "    com_1_not_verified = list()\n",
    "    \n",
    "    sentiment_df_0 = pd.DataFrame()\n",
    "    sentiment_df_1 = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for node in covid_metadata.nodes(data=True):\n",
    "        if node[1] != {} and 'verified' in node[1]:\n",
    "            #print(node)\n",
    "            if covid_compact.nodes[node[0]]['sentimentComm'] == 0:\n",
    "                if node[1]['verified'] == 'True':\n",
    "                    com_0_verified = add_to_list(com_0_verified, covid_compact.nodes[node[0]]['sentiment'])\n",
    "                else:\n",
    "                    com_0_not_verified = add_to_list(com_0_not_verified, covid_compact.nodes[node[0]]['sentiment'])\n",
    "            else:\n",
    "                if node[1]['verified'] == 'True':\n",
    "                    com_1_verified = add_to_list(com_1_verified, covid_compact.nodes[node[0]]['sentiment'])\n",
    "                else:\n",
    "                    com_1_not_verified = add_to_list(com_1_not_verified, covid_compact.nodes[node[0]]['sentiment'])\n",
    "    # verified\n",
    "    if opt == 0:\n",
    "        sentiment_df_0['sentiment'] = com_0_verified\n",
    "        sentiment_df_1['sentiment'] = com_1_verified\n",
    "        title = 'Distribuzione sentiment score per utenti verificati'\n",
    "    # not verified\n",
    "    elif opt == 1:\n",
    "        sentiment_df_0['sentiment'] = com_0_not_verified\n",
    "        sentiment_df_1['sentiment'] = com_1_not_verified\n",
    "        title = 'Distribuzione sentiment score per utenti non verificati'\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = sns.distplot(sentiment_df_0['sentiment'], color='b', hist=False, label='Community 0')\n",
    "    ax = sns.distplot(sentiment_df_1['sentiment'], color='r', hist=False, label='Community 1')\n",
    "    ax.set_title(title, fontsize=15)\n",
    "    ax.set_xlabel('Score sentimento', fontsize=15)\n",
    "    ax.set_ylabel('Distribuzione probabilità', fontsize=15)\n",
    "    ax.xaxis.set_tick_params(labelsize=15)\n",
    "    ax.yaxis.set_tick_params(labelsize=15)\n",
    "    ax.legend(prop={\"size\":15})\n",
    "    save_img(title, 'COVID-19')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def add_to_list(total, partial):\n",
    "    total.append(partial)\n",
    "    return total\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def add_stop_words_covid(start_stop_words):\n",
    "    start_stop_words.append('coronavirus')\n",
    "    start_stop_words.append('covid coronavirus')\n",
    "    start_stop_words.append('covid')\n",
    "    start_stop_words.append('outbreak')\n",
    "    start_stop_words.append('ncov')\n",
    "    start_stop_words.append('ncov ncov')\n",
    "    start_stop_words.append('coronavirus covid')\n",
    "    start_stop_words.append('cases coronavirus')\n",
    "    start_stop_words.append('coronavirus coronaviru')\n",
    "    start_stop_words.append('coronavirus spread')\n",
    "    start_stop_words.append('coronavirus case')\n",
    "    start_stop_words.append('coronavirus outbreak')\n",
    "    start_stop_words.append('coronaviru')\n",
    "    start_stop_words.append('corona viru')\n",
    "    start_stop_words.append('viru')\n",
    "    start_stop_words.append('corona')\n",
    "    start_stop_words.append('china coronavirus')\n",
    "    start_stop_words.append('wuhan china')\n",
    "    start_stop_words.append('china')\n",
    "    start_stop_words.append('China')\n",
    "    start_stop_words.append('confirmed case')\n",
    "    start_stop_words.append('confirmed')\n",
    "    start_stop_words.append('case')\n",
    "    start_stop_words.append('novel coronavirus')\n",
    "    start_stop_words.append('novel')\n",
    "    start_stop_words.append('coronavirus')\n",
    "    start_stop_words.append('Coronavirus')\n",
    "    start_stop_words.append('cases')\n",
    "    start_stop_words.append('COVID')\n",
    "    start_stop_words.append('breaking')\n",
    "    start_stop_words.append('CoronavirusOutbreak')\n",
    "    start_stop_words.append('Wuhan')\n",
    "    start_stop_words.append('BREAKING')\n",
    "    start_stop_words.append('news')\n",
    "    start_stop_words.append('virus')\n",
    "    start_stop_words.append('says')\n",
    "    start_stop_words.append('chinese')\n",
    "    start_stop_words.append('deaths')\n",
    "    start_stop_words.append('live')\n",
    "    start_stop_words.append('infected')\n",
    "    start_stop_words.append('reports')\n",
    "    return start_stop_words\n",
    "\n",
    "def add_stop_words_vaccination(start_stop_words):\n",
    "    start_stop_words.append('vaccination')\n",
    "    start_stop_words.append('vaccine')\n",
    "    start_stop_words.append('vaccines')\n",
    "    start_stop_words.append('diseases')\n",
    "    start_stop_words.append('vaccine')\n",
    "    start_stop_words.append('vaccines')\n",
    "    start_stop_words.append('vaccinated')\n",
    "    start_stop_words.append('measles')\n",
    "    start_stop_words.append('forced')\n",
    "    start_stop_words.append('like')\n",
    "    start_stop_words.append('rates')\n",
    "    start_stop_words.append('outbreaks')\n",
    "    start_stop_words.append('getting')\n",
    "    start_stop_words.append('know')\n",
    "    start_stop_words.append('want')\n",
    "    start_stop_words.append('school')\n",
    "    start_stop_words.append('cases')\n",
    "    start_stop_words.append('think')\n",
    "    start_stop_words.append('year')\n",
    "    start_stop_words.append('polio')\n",
    "    start_stop_words.append('people')\n",
    "    start_stop_words.append('rabies')\n",
    "    start_stop_words.append('today')\n",
    "    start_stop_words.append('years')\n",
    "    start_stop_words.append('work')\n",
    "    start_stop_words.append('infection')\n",
    "    start_stop_words.append('ebola')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return start_stop_words\n",
    "\n",
    "def clean_data(tweets, name):\n",
    "    with open(\"/Users/villons/Desktop/echo-chamers/src/preprocessing/stopwords.txt\", \"rb\") as fp:\n",
    "        stop_words = pickle.load(fp) \n",
    "    if name == 'COVID-19':\n",
    "        stop_words = add_stop_words_covid(stop_words)\n",
    "    elif name == 'Vaccination':\n",
    "        stop_words = add_stop_words_vaccination(stop_words)\n",
    "    \n",
    "    tweets = delete_url(list(set(tweets)))\n",
    "    tokening = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    tweet_series = pd.Series(tweets)\n",
    "    tweets_tokenized = tweet_series.apply(tokening.tokenize)\n",
    "    tweets_tokenized_stop = tweets_tokenized.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    list_done = list(tweets_tokenized_stop)\n",
    "    clean_tweet = list()\n",
    "    for tweet in tqdm(list_done, desc='tweet recomposed'):\n",
    "        sentence = ''\n",
    "        for word in tweet:\n",
    "            sentence += f'{word} '\n",
    "        clean_tweet.append(sentence)\n",
    "    df = pd.DataFrame(np.array(clean_tweet).reshape(len(clean_tweet),1), columns = ['Tweet'])\n",
    "    processed_docs = df['Tweet'].map(preprocess)\n",
    "    processed_docs = processed_docs.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    return processed_docs\n",
    "\n",
    "def tweet_word_cloud(graph, title, comm_number, name, com_type = 'sentimentComm'):\n",
    "    tweet_com = list()\n",
    "\n",
    "    \n",
    "    for edge in graph.edges(data=True):\n",
    "        if comm_number != 2:\n",
    "            if (graph.nodes[edge[0]][com_type] == graph.nodes[edge[1]][com_type]\n",
    "                and graph.nodes[edge[0]][com_type] == comm_number):\n",
    "                    tweet_com.append(edge[2]['tweets'])\n",
    "                    if name == 'Vaccination':\n",
    "                        tweet_com.append(edge[2]['hashtags'])\n",
    "        else:\n",
    "            if graph.nodes[edge[0]][com_type] != graph.nodes[edge[1]][com_type]:\n",
    "                tweet_com.append(edge[2]['tweets'])\n",
    "            \n",
    "    \n",
    "    tweet_com = [item for sublist in tweet_com for item in sublist]\n",
    "    \n",
    "    processed_docs = clean_data(tweet_com, name)\n",
    "    \n",
    "    # return processed_docs\n",
    "\n",
    "    clean = [item for sublist in processed_docs for item in sublist]\n",
    "    \n",
    "    print(len(clean))\n",
    "    if name == 'Vaccination':\n",
    "        wordcloud, colors = word_cloud(' '.join(clean), 1)\n",
    "    else:\n",
    "        wordcloud, colors = word_cloud(' '.join(clean), comm_number)\n",
    "    fig, ax = plt.subplots(figsize=[20,20])\n",
    "        \n",
    "    plt.title(title)\n",
    "    ax.axis(\"off\")\n",
    "    if comm_number == 0:\n",
    "        ax.imshow(wordcloud.recolor(color_func=blue_color_func), interpolation=\"bilinear\")\n",
    "    elif comm_number == 1:\n",
    "        ax.imshow(wordcloud.recolor(color_func=red_color_func), interpolation=\"bilinear\")\n",
    "    elif comm_number == 2:\n",
    "        ax.imshow(wordcloud.recolor(color_func=grey_color_func), interpolation=\"bilinear\")\n",
    "    save_img(title, name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_counter(df):\n",
    "    sentences = (list(itertools.chain(df)))\n",
    "    flat_list = [item for sublist in sentences for item in sublist]\n",
    "    c = Counter(flat_list)\n",
    "    return c\n",
    "\n",
    "def plot_graph_degree_distribution(graph, title, name):\n",
    "    degree_distribution = pd.DataFrame(columns=['degree_value'])\n",
    "    colors = ['#b50000', '#0045b5']\n",
    "        \n",
    "    for node in tqdm(graph.nodes(data=True)):\n",
    "        edges = list(graph.edges(node[0], data=True))\n",
    "        edge_degree = 0\n",
    "        for edge in edges:\n",
    "            edge_degree += 1\n",
    "        degree_distribution.loc[len(degree_distribution)] = [int(edge_degree)]\n",
    "        \n",
    "    degree_value = degree_distribution['degree_value']\n",
    "    removed_outliers = degree_value.between(degree_value.quantile(.15), degree_value.quantile(0.85))\n",
    "    index_names = degree_distribution[~removed_outliers].index\n",
    "    degree_distribution.drop(index_names, inplace=True)\n",
    "    degree_distribution['degree_value'] = degree_distribution['degree_value'].apply(lambda x: int(x))\n",
    "    \n",
    "    x = list(dict(degree_distribution['degree_value'].value_counts()).keys())\n",
    "    \n",
    "    y = list(dict(degree_distribution['degree_value'].value_counts()).values())\n",
    "    \n",
    "    y = y/sum(y)\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #ax = sns.distplot(degree_distribution['degree_value'], hist=False, kde=True)\n",
    "    #ax = ax.hist(degree_distribution['degree_value'], edgecolor = 'black', density=True)\n",
    "    ax = ax.bar(x, y)\n",
    "    plt.xticks(ticks=x, labels=x, fontsize=15)\n",
    "    plt.yticks(ticks=[0.0, 0.1, 0.2, 0.3], labels=[0.0, 0.1, 0.2, 0.3], fontsize=15)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.xlabel('Grado', fontsize=15)\n",
    "    plt.ylabel('P(Grado)', fontsize=15)\n",
    "    #ax.yaxis.set_tick_params(labelsize=15)\n",
    "    \n",
    "    \n",
    "    save_img(title, name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garimella\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/villons/Desktop/echo-chamers/src/data/garimella_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beefban = nx.read_gml('./Graph/beefban.gml')\n",
    "ukraine = nx.read_gml('./Graph/ukraine.gml')\n",
    "kissing_day = nx.read_gml('./Graph/nationalkissingday.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centrality analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_0 = extract_community(kissing_day, 0, 'weightComm')\n",
    "comm_1 = extract_community(kissing_day, 1, 'weightComm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_degree_and_betweenness(comm_0, 0)\n",
    "compute_degree_and_betweenness(comm_1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie chart community membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_pie_chart(beefban, 'weightComm', 'BeefBan con approccio strutturale', 'Garimella')\n",
    "community_pie_chart(ukraine, 'weightComm', 'Ukraine con approccio strutturale', 'Garimella')\n",
    "community_pie_chart(kissing_day, 'weightComm', 'Kissingday con approccio strutturale', 'Garimella')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_distribution(beefban, 'Garimella', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_distribution(ukraine, 'Garimella', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_distribution(kissing_day, 'Garimella', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/villons/Desktop/echo-chamers/src/data/corona_virus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_compact = nx.read_gml('./Graph/Final_Graph_Covid.gml')\n",
    "covid_complete = nx.read_gml('./Graph/Final_DiGraph_Covid.gml')\n",
    "covid_metadata = nx.read_gml('./Graph/Final_DiGraph_Covid_data.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(covid_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_com_id(node, com_type):\n",
    "    if node[com_type] == 1:\n",
    "        node[com_type] = 0\n",
    "    else:\n",
    "        node[com_type] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in covid_compact.nodes(data=True):\n",
    "    change_com_id(covid_compact.nodes[node[0]], 'weightComm')\n",
    "    change_com_id(covid_compact.nodes[node[0]], 'sentimentComm')\n",
    "    \n",
    "    change_com_id(covid_complete.nodes[node[0]], 'weightComm')\n",
    "    change_com_id(covid_complete.nodes[node[0]], 'sentimentComm')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in covid_compact.nodes(data=True):\n",
    "    covid_metadata.nodes[node[0]]['sentimentComm'] = node[1]['sentimentComm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_19_df = create_graph_df(covid_compact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_list(total, partial):\n",
    "    total.append(partial)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sent_ver_or_not(covid_compact, covid_metadata, opt):    \n",
    "    com_0_verified = list()\n",
    "    com_0_not_verified = list()\n",
    "\n",
    "    com_1_verified = list()\n",
    "    com_1_not_verified = list()\n",
    "    \n",
    "    sentiment_df_0 = pd.DataFrame()\n",
    "    sentiment_df_1 = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for node in covid_metadata.nodes(data=True):\n",
    "        if node[1] != {}:\n",
    "            if covid_compact.nodes[node[0]]['sentimentComm'] == 0:\n",
    "                if node[1]['verified'] == 'True':\n",
    "                    com_0_verified = add_to_list(com_0_verified, covid_compact.nodes[node[0]]['sentiment'])\n",
    "                else:\n",
    "                    com_0_not_verified = add_to_list(com_0_not_verified, covid_compact.nodes[node[0]]['sentiment'])\n",
    "            else:\n",
    "                if node[1]['verified'] == 'True':\n",
    "                    com_1_verified = add_to_list(com_1_verified, covid_compact.nodes[node[0]]['sentiment'])\n",
    "                else:\n",
    "                    com_1_not_verified = add_to_list(com_1_not_verified, covid_compact.nodes[node[0]]['sentiment'])\n",
    "    # verified\n",
    "    if opt == 0:\n",
    "        sentiment_df_0['sentiment'] = com_0_verified\n",
    "        sentiment_df_1['sentiment'] = com_1_verified\n",
    "        title = 'Distribuzione sentiment score per utenti verificati'\n",
    "    # not verified\n",
    "    elif opt == 1:\n",
    "        sentiment_df_0['sentiment'] = com_0_not_verified\n",
    "        sentiment_df_1['sentiment'] = com_1_not_verified\n",
    "        title = 'Distribuzione sentiment score per utenti non verificati'\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = sns.distplot(sentiment_df_0['sentiment'], color='b', hist=False, label='Community 0')\n",
    "    ax = sns.distplot(sentiment_df_1['sentiment'], color='r', hist=False, label='Community 1')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Score sentimento')\n",
    "    ax.set_ylabel('Distribuzione probabilità')\n",
    "    ax.legend()\n",
    "    save_img(title, 'COVID-19')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sent_ver_or_not(covid_compact, covid_metadata, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sent_ver_or_not(covid_compact, covid_metadata, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centrality analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(comm_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_0 = extract_community(covid_complete, 0, 'sentimentComm')\n",
    "comm_1 = extract_community(covid_complete, 1, 'sentimentComm')\n",
    "compute_degree_and_betweenness(comm_0, 1)\n",
    "compute_degree_and_betweenness(comm_1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie chart community membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "community_pie_chart(covid_compact, 'weightComm', 'COVID-19 con approccio strutturale', 'COVID-19')\n",
    "community_pie_chart(covid_compact, 'sentimentComm', 'COVID-19 con approccio sentiment e strutturale', 'COVID-19')\n",
    "community_pie_chart(covid_compact, 'topicComm', 'COVID-19 con approccio topic e strutturale', 'COVID-19')\n",
    "community_pie_chart(covid_compact, 'hybridComm', 'COVID-19 con approccio ibrido', 'COVID-19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution plot different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_distribution(covid_metadata, 'COVID-19', covid_19_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic talking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_distribution(covid_19_df, 'COVID-19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_distribution(covid_metadata, 'COVID-19', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_snsdist(metadata, 'numFollowers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_snsdist(metadata, 'numLikes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_snsdist(metadata, 'numStatuses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_pie_chart(\"0\", metadata)\n",
    "verified_pie_chart(\"1\", metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (metadata['created'] >= '2020')\n",
    "metadata_2020 = metadata.loc[filt]\n",
    "default_pie_chart('0', metadata_2020, 'DefaultImage', 1)\n",
    "default_pie_chart('1', metadata_2020, 'DefaultImage', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist_2020(metadata_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary_word_cloud(metadata, 'Words cloud sulle descrizione dei profili utenti data tutto il grafo CODIV-19', 0)\n",
    "comm_0 = metadata.query('community == 0')\n",
    "summary_word_cloud(comm_0, 'Words cloud sulle descrizione dei profili utenti data tutto il grafo CODIV-19 communty 0', 1)\n",
    "comm_1 = metadata.query('community == 1')\n",
    "summary_word_cloud(comm_1, 'Words cloud sulle descrizione dei profili utenti data tutto il grafo CODIV-19 community 1', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_word_cloud(covid_metadata, 'Wordcloud tweet intra community 0', 0, 'COVID-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_word_cloud(covid_metadata, 'Wordcloud tweet intra community 1', 1, 'COVID-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_word_cloud(covid_metadata, 'Wordcloud tweet extra community', 2, 'COVID-19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/villons/Desktop/echo-chamers/src/data/vax_no_vax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vax_compact = nx.read_gml('Graph/Final_Graph_vax.gml')\n",
    "vax_complete = nx.read_gml('Graph/Final_DiGraph_Vax.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(vax_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccination_df = create_graph_df(vax_compact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(vax_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_0 = extract_community(vax_complete, 0, 'sentimentComm')\n",
    "comm_1 = extract_community(vax_complete, 1, 'sentimentComm')\n",
    "compute_degree_and_betweenness(comm_0, 1)\n",
    "compute_degree_and_betweenness(comm_1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie chart community membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_pie_chart(vax_compact, 'weightComm', 'Vaccinazioni con approccio strutturale', 'Vaccination')\n",
    "community_pie_chart(vax_compact, 'sentimentComm', 'Vaccinazioni con approccio sentiment e strutturale', 'Vaccination')\n",
    "community_pie_chart(vax_compact, 'topicComm', 'Vaccinazioni con approccio topic e strutturale', 'Vaccination')\n",
    "community_pie_chart(vax_compact, 'hybridComm', 'Vaccinazioni con approccio ibrido', 'Vaccination')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution plot different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_distribution(vax_compact, 'Vaccination', vaccination_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic talking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution(vaccination_df, 'Vaccination')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_distribution(vax_complete, 'Vaccination', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def add_stop_words_covid(start_stop_words):\n",
    "    start_stop_words.append('coronavirus')\n",
    "    start_stop_words.append('covid coronavirus')\n",
    "    start_stop_words.append('covid')\n",
    "    start_stop_words.append('outbreak')\n",
    "    start_stop_words.append('ncov')\n",
    "    start_stop_words.append('ncov ncov')\n",
    "    start_stop_words.append('coronavirus covid')\n",
    "    start_stop_words.append('cases coronavirus')\n",
    "    start_stop_words.append('coronavirus coronaviru')\n",
    "    start_stop_words.append('coronavirus spread')\n",
    "    start_stop_words.append('coronavirus case')\n",
    "    start_stop_words.append('coronavirus outbreak')\n",
    "    start_stop_words.append('coronaviru')\n",
    "    start_stop_words.append('corona viru')\n",
    "    start_stop_words.append('viru')\n",
    "    start_stop_words.append('corona')\n",
    "    start_stop_words.append('china coronavirus')\n",
    "    start_stop_words.append('wuhan china')\n",
    "    start_stop_words.append('china')\n",
    "    start_stop_words.append('China')\n",
    "    start_stop_words.append('confirmed case')\n",
    "    start_stop_words.append('confirmed')\n",
    "    start_stop_words.append('case')\n",
    "    start_stop_words.append('novel coronavirus')\n",
    "    start_stop_words.append('novel')\n",
    "    start_stop_words.append('coronavirus')\n",
    "    start_stop_words.append('Coronavirus')\n",
    "    start_stop_words.append('cases')\n",
    "    start_stop_words.append('COVID')\n",
    "    start_stop_words.append('breaking')\n",
    "    start_stop_words.append('CoronavirusOutbreak')\n",
    "    start_stop_words.append('Wuhan')\n",
    "    start_stop_words.append('BREAKING')\n",
    "    start_stop_words.append('news')\n",
    "    start_stop_words.append('virus')\n",
    "    start_stop_words.append('says')\n",
    "    start_stop_words.append('chinese')\n",
    "    start_stop_words.append('deaths')\n",
    "    start_stop_words.append('live')\n",
    "    start_stop_words.append('infected')\n",
    "    start_stop_words.append('reports')\n",
    "    return start_stop_words\n",
    "\n",
    "def add_stop_words_vaccination(start_stop_words):\n",
    "    start_stop_words.append('vaccination')\n",
    "    start_stop_words.append('vaccine')\n",
    "    start_stop_words.append('vaccines')\n",
    "    start_stop_words.append('diseases')\n",
    "    start_stop_words.append('vaccine')\n",
    "    start_stop_words.append('vaccines')\n",
    "    start_stop_words.append('vaccinated')\n",
    "    start_stop_words.append('measles')\n",
    "    start_stop_words.append('forced')\n",
    "    start_stop_words.append('like')\n",
    "    start_stop_words.append('rates')\n",
    "    start_stop_words.append('outbreaks')\n",
    "    start_stop_words.append('getting')\n",
    "    start_stop_words.append('know')\n",
    "    start_stop_words.append('want')\n",
    "    start_stop_words.append('school')\n",
    "    start_stop_words.append('cases')\n",
    "    start_stop_words.append('think')\n",
    "    start_stop_words.append('year')\n",
    "    start_stop_words.append('polio')\n",
    "    start_stop_words.append('people')\n",
    "    start_stop_words.append('rabies')\n",
    "    start_stop_words.append('today')\n",
    "    start_stop_words.append('years')\n",
    "    start_stop_words.append('work')\n",
    "    start_stop_words.append('infection')\n",
    "    start_stop_words.append('ebola')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return start_stop_words\n",
    "\n",
    "def clean_data(tweets, name):\n",
    "    with open(\"/Users/villons/Desktop/echo-chamers/src/preprocessing/stopwords.txt\", \"rb\") as fp:\n",
    "        stop_words = pickle.load(fp) \n",
    "    if name == 'COVID-19':\n",
    "        stop_words = add_stop_words_covid(stop_words)\n",
    "    elif name == 'Vaccination':\n",
    "        stop_words = add_stop_words_vaccination(stop_words)\n",
    "    \n",
    "    tweets = delete_url(list(set(tweets)))\n",
    "    tokening = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    tweet_series = pd.Series(tweets)\n",
    "    tweets_tokenized = tweet_series.apply(tokening.tokenize)\n",
    "    tweets_tokenized_stop = tweets_tokenized.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    list_done = list(tweets_tokenized_stop)\n",
    "    clean_tweet = list()\n",
    "    for tweet in tqdm(list_done, desc='tweet recomposed'):\n",
    "        sentence = ''\n",
    "        for word in tweet:\n",
    "            sentence += f'{word} '\n",
    "        clean_tweet.append(sentence)\n",
    "    df = pd.DataFrame(np.array(clean_tweet).reshape(len(clean_tweet),1), columns = ['Tweet'])\n",
    "    processed_docs = df['Tweet'].map(preprocess)\n",
    "    processed_docs = processed_docs.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    return processed_docs\n",
    "\n",
    "def tweet_word_cloud(graph, title, comm_number, name, com_type = 'sentimentComm'):\n",
    "    tweet_com = list()\n",
    "\n",
    "    \n",
    "    for edge in graph.edges(data=True):\n",
    "        if comm_number != 2:\n",
    "            if (graph.nodes[edge[0]][com_type] == graph.nodes[edge[1]][com_type]\n",
    "                and graph.nodes[edge[0]][com_type] == comm_number):\n",
    "                    tweet_com.append(edge[2]['tweets'])\n",
    "                    if name == 'Vaccination':\n",
    "                        tweet_com.append(edge[2]['hashtags'])\n",
    "        else:\n",
    "            if graph.nodes[edge[0]][com_type] != graph.nodes[edge[1]][com_type]:\n",
    "                tweet_com.append(edge[2]['tweets'])\n",
    "            \n",
    "    \n",
    "    tweet_com = [item for sublist in tweet_com for item in sublist]\n",
    "    \n",
    "    processed_docs = clean_data(tweet_com, name)\n",
    "    \n",
    "    # return processed_docs\n",
    "\n",
    "    clean = [item for sublist in processed_docs for item in sublist]\n",
    "    \n",
    "    print(len(clean))\n",
    "    if name == 'Vaccination':\n",
    "        wordcloud, colors = word_cloud(' '.join(clean), 1)\n",
    "    else:\n",
    "        wordcloud, colors = word_cloud(' '.join(clean), comm_number)\n",
    "    fig, ax = plt.subplots(figsize=[20,20])\n",
    "        \n",
    "    plt.title(title)\n",
    "    ax.axis(\"off\")\n",
    "    if comm_number == 0:\n",
    "        ax.imshow(wordcloud.recolor(color_func=blue_color_func), interpolation=\"bilinear\")\n",
    "    elif comm_number == 1:\n",
    "        ax.imshow(wordcloud.recolor(color_func=red_color_func), interpolation=\"bilinear\")\n",
    "    elif comm_number == 2:\n",
    "        ax.imshow(wordcloud.recolor(color_func=grey_color_func), interpolation=\"bilinear\")\n",
    "    save_img(title, name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_counter(df):\n",
    "    sentences = (list(itertools.chain(df)))\n",
    "    flat_list = [item for sublist in sentences for item in sublist]\n",
    "    c = Counter(flat_list)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_word_cloud(vax_complete, 'Wordcloud tweet intra community 0', 0, 'Vaccination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_word_cloud(vax_complete, 'Wordcloud tweet intra community 1', 1, 'Vaccination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_word_cloud(vax_complete, 'Wordcloud tweet extra community', 2, 'Vaccination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "echo-chamber",
   "language": "python",
   "name": "echo-chamber"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
